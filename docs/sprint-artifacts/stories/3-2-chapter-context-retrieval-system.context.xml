<story-context id="3-2-chapter-context-retrieval-system" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>3-2</storyId>
    <title>章节上下文检索系统实现</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-17</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>/home/abelxiaoxing/work/InfiniteQuill/docs/sprint-artifacts/3-2-chapter-context-retrieval-system.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>小说创作者</asA>
    <iWant>新生成的章节与之前章节保持上下文连贯</iWant>
    <soThat>我的小说故事情节自然流畅</soThat>
    <tasks>
      任务1: ChromaDB集合和元数据准备
      - 在vectorstore_utils.py中查找现有的ChromaDB集合
      - 验证章节内容是否以带元数据的方式存储
      - 如果缺少,添加metadata字段: chapter_num, project_id, content_type
      - 测试元数据过滤查询
      - 验证不同小说项目的隔离性

      任务2: 章节摘要向量生成
      - 集成sentence-transformers库
      - 选择合适的中文文本嵌入模型(e.g., paraphrase-multilingual-MiniLM-L12-v2)
      - 在生成章节后,创建章节摘要或关键段落向量
      - 将向量存储到ChromaDB
      - 验证向量质量和检索效果

      任务3: 上下文检索函数实现
      - 在knowledge.py或新文件context_retriever.py中创建retrieve_context()函数
      - 实现基于当前章节的语义向量查询
      - 添加metadata过滤条件(chapter_num=N-1, project_id=current_project)
      - 实现k参数控制(默认值3,可动态调整)
      - 返回最相关的k个段落及其相似度分数

      任务4: Token限制和动态调整
      - 计算检索到的段落总token数(使用tiktoken或类似库)
      - 如果超过max_context_tokens(2000),减少k值重新检索
      - 如果仍然超出,对段落进行摘要或截断
      - 确保最终上下文在token限制内
      - 记录实际使用的token数到日志

      任务5: 提示词构建集成
      - 在chapter.py的generate_chapter()函数中集成上下文检索
      - 在prompt模板中添加"前文相关信息:"部分
      - 将检索到的段落格式化为清晰的上下文块
      - 确保上下文与当前章节的蓝图信息整合
      - 测试提示词的最终格式

      任务6: 检索质量评估和调优
      - 创建测试集: 10-20个章节对,人工标注相关性
      - 运行检索系统,计算recall@k和precision@k
      - 分析检索失败案例,调整嵌入模型或参数
      - 测试不同k值(1, 3, 5)对生成质量的影响
      - 选择最优k值作为默认值

      任务7: 完整集成和端到端测试
      - 集成所有模块: 向量生成 → 存储 → 检索 → 提示词构建 → 生成
      - 测试生成第2章时是否包含第1章上下文
      - 测试生成第5章时是否包含第4章上下文
      - 验证多章节小说的连贯性提升
      - 性能测试: 上下文检索不应该显著增加生成时间
      - 与Story 3.1的名字一致性机制集成测试
    </tasks>
  </story>

  <acceptanceCriteria>
    1. 前章节内容检索 - Given 生成第N章内容时(N>1), When 开始检索上下文, Then 应该自动从第N-1章中提取关键内容段落, And 不应该检索不相关章节的内容
    2. 向量相似度匹配 - Given 已有前章节内容, When 使用向量检索查找最相关段落时, Then 应该基于内容语义相似度计算匹配分数, And 返回分数最高的前k个段落(默认k=3)
    3. 上下文集成到提示词 - Given 已检索到相关的前章节段落, When 构建LLM提示词时, Then 应该将这些段落作为上下文添加到提示词中, And 上下文应该以清晰的格式标注(如"前文相关信息:")
    4. Token限制管理 - Given 检索到的前章节内容, When 添加到提示词时, Then 应该遵守max_context_tokens=2000的限制, And 如果超出限制应该动态调整检索段落数k值或减少每个段落长度
    5. ChromaDB元数据过滤 - Given 使用ChromaDB进行向量检索, When 查询时, Then 应该使用metadata过滤确保只检索相关章节(如chapter_num=N-1), And 不应该跨小说项目检索(使用project_id过滤)
    6. 上下文质量评估 - Given 检索结果返回, When 评估检索质量时, Then 检索到的段落应该与当前章节主题相关, And 不应该包含无关或重复信息
  </acceptanceCriteria>

  <artifacts>
    <docs>
      /home/abelxiaoxing/work/InfiniteQuill/docs/epics.md#Epic-3
      /home/abelxiaoxing/work/InfiniteQuill/docs/bmm-architecture-decisions-2025-11-16.md#需求2
      /home/abelxiaoxing/work/InfiniteQuill/novel_generator/CLAUDE.md
    </docs>
    <code>
      <!-- 主要文件位置 -->
      /home/abelxiaoxing/work/InfiniteQuill/novel_generator/knowledge.py
      <!-- 核心函数: import_knowledge_file(), advanced_split_content() -->
      <!-- 当前功能: 知识库管理和向量检索基础 -->

      /home/abelxiaoxing/work/InfiniteQuill/novel_generator/chapter.py
      <!-- 核心函数: generate_chapter_draft(), build_chapter_prompt(), summarize_recent_chapters() -->
      <!-- 当前功能: 章节生成主逻辑，包含前文摘要和知识库检索 -->

      /home/abelxiaoxing/work/InfiniteQuill/novel_generator/vectorstore_utils.py
      <!-- 核心函数: get_relevant_context_from_vector_store(), update_vector_store(), split_text_for_vectorstore() -->
      <!-- 当前功能: ChromaDB操作封装，支持文档分块和相似度检索 -->

      /home/abelxiaoxing/work/InfiniteQuill/prompt_definitions.py
      <!-- 核心提示词: next_chapter_draft_prompt, summarize_recent_chapters_prompt, knowledge_search_prompt -->
      <!-- 当前功能: 章节生成和知识检索的提示词模板 -->

      /home/abelxiaoxing/work/InfiniteQuill/llm_adapters.py
      <!-- 核心类: BaseLLMAdapter, create_llm_adapter() -->
      <!-- 当前功能: 统一LLM接口适配器，支持多种API提供商 -->

      /home/abelxiaoxing/work/InfiniteQuill/embedding_adapters.py
      <!-- 核心类: BaseEmbeddingAdapter, create_embedding_adapter() -->
      <!-- 当前功能: 统一嵌入接口适配器，支持多种嵌入服务 -->

      <!-- 现有代码片段 -->
      <!-- 当前章节检索逻辑 (chapter.py lines 363-392) -->
      recent_texts = get_last_n_chapters_text(chapters_dir, novel_number, n=3)
      short_summary = summarize_recent_chapters(...)
      previous_excerpt = get_last_n_chapters_text(...)[-800:]

      <!-- 当前向量存储结构 (vectorstore_utils.py lines 182-210) -->
      def update_vector_store(embedding_adapter, new_chapter: str, filepath: str):
          splitted_texts = split_text_for_vectorstore(new_chapter)
          docs = [Document(page_content=str(t)) for t in splitted_texts]
          store.add_documents(docs) # 缺少metadata支持

      <!-- 当前向量检索函数 (vectorstore_utils.py lines 211-234) -->
      def get_relevant_context_from_vector_store(embedding_adapter, query: str, filepath: str, k: int = 2) -> str:
          docs = store.similarity_search(query, k=k) # 无metadata过滤
          combined = "\n".join([d.page_content for d in docs])
          return combined[:2000] # 基础token限制
    </code>
    <dependencies>
      <!-- 主要技术依赖 -->
      ChromaDB 1.0.20 - 向量数据库，用于存储和检索章节向量
      sentence-transformers - 多语言文本嵌入模型，推荐使用paraphrase-multilingual-MiniLM-L12-v2
      tiktoken - OpenAI token计数库（或其他LLM提供商的等效库）
      langchain-chroma - ChromaDB与LangChain集成
      langchain-openai - OpenAI与LangChain集成
      nltk - 文本分句和预处理
      scikit-learn - 余弦相似度计算

      <!-- 模型选择建议 -->
      推荐嵌入模型: paraphrase-multilingual-MiniLM-L12-v2
      - 优点: 多语言支持，模型小(110MB)，速度快
      - 缺点: 中文语义理解可能不如专用模型
      - 替代方案: text2vec-large-chinese（更大但中文效果更好）
    </dependencies>
  </artifacts>

  <constraints>
    <!-- 性能约束 -->
    - 向量检索不应该阻塞主生成流程，应该异步执行
    - 上下文应该作为可选增强，不应该降低无上下文时的质量
    - 需要处理第一个章节(无前文)的边界情况
    - 跨项目隔离: 不应该检索其他小说项目的内容
    - Token限制: max_context_tokens=2000，需要动态调整策略

    <!-- 技术约束 -->
    - 使用现有的ChromaDB基础设施，保持向后兼容性
    - 嵌入模型选择要考虑性能和效果的平衡
    - 需要支持多种LLM提供商的token计算
    - 保持与Story 3.1名字一致性机制的兼容性

    <!-- 质量约束 -->
    - 检索到的上下文必须与当前章节主题相关
    - 相似度阈值: 最低0.6分才使用检索结果
    - 上下文内容不应该包含无关或重复信息
    - 需要支持人工评估和质量监控
  </constraints>

  <interfaces>
    <!-- ChromaDB向量存储接口 -->
    - store.similarity_search_with_score() - 带分数的相似度搜索
    - store.similarity_search_with_relevance_scores() - 相关性分数搜索
    - collection.get() - 获取带metadata的文档
    - collection.query() - 复杂查询包括metadata过滤

    <!-- 嵌入适配器接口 -->
    - embedding_adapter.embed_documents(texts) - 批量文档嵌入
    - embedding_adapter.embed_query(query) - 查询嵌入
    - create_embedding_adapter() - 工厂函数创建适配器

    <!-- LLM适配器接口 -->
    - llm_adapter.invoke(prompt) - LLM调用接口
    - create_llm_adapter() - 工厂函数创建适配器

    <!-- token计算接口 -->
    - tiktoken.encoding_for_model() - 获取模型编码
    - encoding.encode(text) - 文本token化
    - encoding.decode(tokens) - token解码
  </interfaces>

  <tests>
    <standards>
      - 单元测试覆盖率 > 80%
      - 集成测试覆盖主要业务流程
      - 检索质量评估: recall@k > 0.7, precision@k > 0.8
      - 性能测试: 上下文检索延迟 < 2秒
      - Token计算准确性: 误差 < 5%
    </standards>
    <locations>
      - /home/abelxiaoxing/work/InfiniteQuill/tests/test_vectorstore_utils.py
      - /home/abelxiaoxing/work/InfiniteQuill/tests/test_chapter_context_retrieval.py
      - /home/abelxiaoxing/work/InfiniteQuill/tests/test_embedding_quality.py
      - /home/abelxiaoxing/work/InfiniteQuill/tests/test_token_management.py
    </locations>
    <ideas>
      - 创建人工标注的章节对测试数据集
      - 实现A/B测试框架对比有无上下文的效果
      - 构建自动评估指标: BLEU分数、连贯性评分
      - 测试不同k值对生成质量的影响
      - 验证跨项目隔离的有效性
      - 压力测试: 大文档集的检索性能
    </ideas>
  </tests>

  <!-- 向量存储架构设计 -->
  <vector_store>
    <schema>
      <!-- 推荐的metadata结构 -->
      {
        "chapter_num": 1,           # 章节编号
        "project_id": "project_123", # 项目ID，确保隔离
        "content_type": "full",      # 内容类型: full, summary, excerpt
        "timestamp": "2025-11-17",   # 创建时间
        "word_count": 1500,          # 字数统计
        "similarity_score": 0.85     # 如果存储的是相似片段
      }

      <!-- 存储策略 -->
      - 每个章节存储完整内容向量
      - 可选: 存储章节摘要向量作为备选
      - 支持按chapter_num和project_id进行metadata过滤
      - 使用content_type区分不同类型的内容
    </schema>

    <retrieval>
      <algorithms>
        - 主要: cosine similarity (余弦相似度)
        - 备选: dot product (点积) for normalized vectors
        - 支持: 近似最近邻搜索 (HNSW索引)

        <!-- 检索流程 -->
        1. 生成当前章节查询向量
        2. 构建metadata过滤条件: {"chapter_num": N-1, "project_id": current_project}
        3. 执行相似度搜索，获取top-k结果
        4. 计算相似度分数，过滤低质量结果 (< 0.6)
        5. 返回格式化的上下文段落
      </algorithms>
    </retrieval>
  </vector_store>

  <!-- 与Story 3.1的集成点 -->
  <integration>
    <!-- 依赖关系 -->
    - Story 3.1完成(角色名字一致性) → Story 3.2开始(章节上下文连贯性)
    - 两者共同提升多章节小说的整体质量

    <!-- 协同工作 -->
    - Story 3.1确保名字一致性
    - Story 3.2确保情节连贯性
    - 状态反馈机制复用: "正在检索前文..." → "上下文已加载"
    - 验证机制复用: 质量评估和重试策略

    <!-- 集成测试 -->
    - 测试名字一致性和上下文连贯性的综合效果
    - 验证两个机制不会相互冲突
    - 确保整体生成质量提升
  </integration>
</story-context>